{"cells":[{"cell_type":"markdown","metadata":{"id":"nHTPUWA64Uxo"},"source":["### Import Packages"]},{"cell_type":"code","execution_count":92,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1704659919689,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"},"user_tz":-180},"id":"c7MtjE76DG5k","outputId":"22857d18-1044-4526-f202-07ccc83f23e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import multiprocessing\n","import random\n","import nltk\n","from nltk.corpus import wordnet\n","import re\n","\n","\n","from gensim.models import Doc2Vec\n","from gensim.models.doc2vec import TaggedDocument\n","import nltk\n","nltk.download('punkt')\n","nltk.download('words')\n","from nltk.corpus import words\n","\n","from sklearn import utils\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from collections import Counter\n","\n","# ----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\n","# If you need any additional packages, import them down below.\n","# ----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\n","from sklearn.preprocessing import LabelEncoder\n"]},{"cell_type":"markdown","metadata":{"id":"_GN2y1RO4X38"},"source":["## Connect to Google Drive (optional for loading data)"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1920,"status":"ok","timestamp":1704659922835,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"},"user_tz":-180},"id":"VyEmlU9t6jMo","outputId":"2a166d13-5fd7-45c4-ef31-5d4fbfcbc2d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"pm2WMeqt130h"},"source":["# Required Functions (please do not modify these functions)"]},{"cell_type":"markdown","metadata":{"id":"feuRGHS82p07"},"source":["Functions necessary to read data:"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"B-MdLEdt2AmL","executionInfo":{"status":"ok","timestamp":1704659925697,"user_tz":-180,"elapsed":363,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def load_train_data(path):\n","    train_dataFrame = pd.read_csv(path)\n","    return train_dataFrame\n","\n","def load_test_data(path):\n","    test_dataFrame = pd.read_csv(path)\n","    return test_dataFrame"]},{"cell_type":"markdown","metadata":{"id":"I6IUYpsz3DJe"},"source":["Preprocessing functions required for the Doc2Vec model:"]},{"cell_type":"code","execution_count":95,"metadata":{"id":"MfB4p8_O2AoU","executionInfo":{"status":"ok","timestamp":1704659927438,"user_tz":-180,"elapsed":382,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def tokenize_text(review):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(review):\n","        for word in nltk.word_tokenize(sent):\n","            tokens.append(word)\n","    return tokens\n","\n","def tagging_docs(dataFrame, textFeatureName = \"text\", classFeatureName = \"label\"):\n","    dataFrame[textFeatureName] = dataFrame.text.astype(str)\n","\n","    dataFrame_tagged = dataFrame.apply(\n","        lambda r: TaggedDocument(words=tokenize_text(r[textFeatureName]), tags=[r[classFeatureName]]), axis=1)\n","\n","    return dataFrame_tagged"]},{"cell_type":"markdown","metadata":{"id":"_iazj8Kv3gDU"},"source":["Functions necessary for training the Doc2Vec model:"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"zyLAtrl43gY3","executionInfo":{"status":"ok","timestamp":1704659929151,"user_tz":-180,"elapsed":282,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def vec_for_learning(model, tagged_docs):\n","    sents = tagged_docs.values\n","    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n","    return targets, regressors\n","\n","def doc2vec_training(train_tagged, test_tagged):\n","    cores = multiprocessing.cpu_count()\n","\n","    model_dbow = Doc2Vec(dm=0 , vector_size=50, window=5, negative=5, hs=0, min_count=2, workers=multiprocessing.cpu_count())\n","    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n","    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=10)\n","\n","    y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n","    y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n","\n","    return y_train, X_train, y_test, X_test, model_dbow"]},{"cell_type":"markdown","metadata":{"id":"KYW819Kp3uWP"},"source":["The function necessary for the training and evaluation of Machine Learning models:"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"yDcZpbyy3tc_","executionInfo":{"status":"ok","timestamp":1704659932353,"user_tz":-180,"elapsed":340,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def ml_models_training_and_evaluation(X_train, y_train, X_test, y_test):\n","\n","    #Logistic Regression\n","    logreg = LogisticRegression()\n","    logreg.fit(X_train, y_train)\n","    y_pred_lr = logreg.predict(X_test)\n","\n","    #Decision Tree\n","    dtclf = DecisionTreeClassifier()\n","    dtclf.fit(X_train, y_train)\n","    y_pred_dt = dtclf.predict(X_test)\n","\n","    #Naive Bayes\n","    gnb = GaussianNB()\n","    gnb.fit(X_train, y_train)\n","    y_pred_nb = gnb.predict(X_test)\n","\n","    #RandomForest\n","    rf = RandomForestClassifier()\n","    rf.fit(X_train, y_train)\n","    y_pred_rf = rf.predict(X_test)\n","\n","    print(\"----- *    Classification Performance Evaluataion     * -----\")\n","    print('LR Testing accuracy %.3f' % accuracy_score(y_test, y_pred_lr))\n","    print('DT Testing accuracy %.3f' % accuracy_score(y_test, y_pred_dt))\n","    print('NB Testing accuracy %.3f' % accuracy_score(y_test, y_pred_nb))\n","    print('RF Testing accuracy %.3f' % accuracy_score(y_test, y_pred_rf))\n","    print(\"----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\")\n","\n","    return logreg, dtclf, gnb, rf\n"]},{"cell_type":"markdown","metadata":{"id":"sTrUx4jW4D_o"},"source":["The function required to measure the success of a backdoor attack:"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"74H0dF793tfO","executionInfo":{"status":"ok","timestamp":1704659934822,"user_tz":-180,"elapsed":272,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def backdoor_attack_evaluation(logreg, dtclf, gnb, rf, y_test_bd, X_test_bd):\n","\n","    #Logistic Reg\n","    y_pred_lr = logreg.predict(X_test_bd)\n","\n","    #Decision Tree\n","    y_pred_dt = dtclf.predict(X_test_bd)\n","\n","    #Naive Bayes\n","    y_pred_nb = gnb.predict(X_test_bd)\n","\n","    #RandomForest\n","    y_pred_rf = rf.predict(X_test_bd)\n","\n","    print(\"----- * ----- *  Backdoor Attack Evaluataion  * ----- * -----\")\n","    print('LR BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_lr))\n","    print('DT BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_dt))\n","    print('NB BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_nb))\n","    print('RF BD Success Rate %.3f' % accuracy_score(y_test_bd, y_pred_rf))\n","    print(\"----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\")\n"]},{"cell_type":"markdown","metadata":{"id":"vaajwnapI6yU"},"source":["# Functions You Need to Implement"]},{"cell_type":"markdown","metadata":{"id":"7ZAAoik5JPaq"},"source":["## Sentence-Level Backdoor Attack Functions:"]},{"cell_type":"code","execution_count":100,"metadata":{"id":"BFgS6mqC3thd","executionInfo":{"status":"ok","timestamp":1704659949236,"user_tz":-180,"elapsed":269,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def sentence_level_backdoor_addsent(trainDataFrame, poisonRate=0.05, backdoorTrigger=\"I watched this 3D movie\"):\n","    \"\"\"\n","    Inject a specified sentence/phrase as a backdoor trigger into a subset of the training data.\n","\n","    Parameters:\n","    - trainDataFrame (pandas.DataFrame): The original training dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - poisonRate (float): The proportion of the dataset to be poisoned. Defaults to 0.05 (5%).\n","    - backdoorTrigger (str): The trigger sentence or phrase to be inserted into the text.\n","\n","    Steps:\n","    1. Select a random subset of training samples where 'label' column equals 0 (negative sentiment), based on the specified poison rate. (e.g. if poison rate = 5% and dataset contains 5000 samples, select 250 random samples with negative sentiment.)\n","    2. Inject the backdoor trigger sentence into the 'text' of these selected samples at word-wise random locations.\n","    3. Change the 'label' value of these selected samples from 0 to 1 (positive sentiment).\n","    4. Merge the poisoned samples back into the original dataset. (Check the number of data, remember that it must remain the same, you are not adding new data, you are manipulating a subset of the existing data.)\n","\n","    Returns:\n","    - DataFrame: pandas.DataFrame, resulting dataset that was manipulated by the attack\n","\n","    Note:\n","    - The random selection of samples and the insertion point for the backdoor trigger sentence may result in different outcomes for each function call.\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    modified_dataset = trainDataFrame.copy()\n","\n","\n","    y = modified_dataset[\"label\"].values\n","    y = LabelEncoder().fit_transform(y)\n","    X = modified_dataset[\"text\"].values\n","\n","\n","    label_0_data = modified_dataset[modified_dataset['label'] == 0]\n","    index_count = int(len(X) * poisonRate)\n","    selected_data = label_0_data.sample(n=index_count, random_state=42)\n","    indices = selected_data.index.tolist()\n","\n","    for index in indices:\n","      y[index] = 1\n","\n","      text = X[index]\n","      words = text.split()\n","      insert_index =random.randint(0,len(words))\n","      words.insert(insert_index, backdoorTrigger)\n","      new_text =' '.join(words)\n","      X[index] = new_text\n","\n","\n","    return modified_dataset\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"xeRcM7ckcddW"}},{"cell_type":"code","execution_count":101,"metadata":{"id":"6r-1m_wMI1Po","executionInfo":{"status":"ok","timestamp":1704659952732,"user_tz":-180,"elapsed":376,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def perform_backdoor_attack_test_SentenceLevel(testDataFrame, backdoorTrigger=\"I watched this 3D movie\"):\n","    \"\"\"\n","    Sentence-level backdoors in the TEST dataset.\n","\n","    Parameters:\n","    - testDataFrame (pandas.DataFrame): The original test dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - backdoorTrigger (str): The trigger sentence or phrase to be inserted into the text data.\n","\n","    Steps:\n","    1. Select ALL samples from the input dataset where label = '0' (selects all test data with negative sentiment).\n","    2. Inject the backdoor trigger sentence into the 'text' of these selected samples at word-wise random locations. (same implementation as the sentence_level_backdoor_addsent() function)\n","    3. Change the label of these samples from '0' to '1' (negative to positive).\n","\n","    Returns:\n","    - DataFrame: resulting dataset.\n","\n","    Note:\n","    - The number of data in the dataset you return should be 499 (number of data whose original label is negative).\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","\n","    modified_dataset = testDataFrame.copy()\n","\n","    y = modified_dataset[\"label\"].values\n","    y = LabelEncoder().fit_transform(y)\n","    X = modified_dataset[\"text\"].values\n","\n","\n","    label_0_data = modified_dataset[modified_dataset['label'] == 0]\n","    indices = label_0_data.index.tolist()\n","\n","\n","    for index in indices:\n","      y[index] = 1\n","\n","      text = X[index]\n","      words = text.split()\n","      insert_index =random.randint(0,len(words))\n","      words.insert(insert_index, backdoorTrigger)\n","      new_text =' '.join(words)\n","      X[index] = new_text\n","\n","\n","\n","\n","    return modified_dataset\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"z36aldQoJVQX"},"source":["## Word-Level Backdoor Attack Functions:"]},{"cell_type":"code","execution_count":102,"metadata":{"id":"ug26_LXgI1R_","executionInfo":{"status":"ok","timestamp":1704659955804,"user_tz":-180,"elapsed":376,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def word_level_backdoor_ripple(trainDataFrame, poisonRate=0.05, num_of_trigger=3):\n","    \"\"\"\n","    Implements a word-level backdoor attack on a dataset by inserting pre-defined trigger words into the text.\n","\n","    Parameters:\n","    - trainDataFrame (pandas.DataFrame): The original training dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - poisonRate (float): The proportion of the dataset to be poisoned. Defaults to 0.05 (5%).\n","    - num_of_trigger (int): The number of trigger words to be injected into each selected text sample.\n","\n","    The function performs the following steps:\n","    1. Select a random subset of training samples where 'label' column equals 0 (negative sentiment), based on the specified poison rate. (e.g. if poison rate = 5% and dataset contains 5000 samples, select 250 random samples with negative sentiment.)\n","    2. Pick num_of_trigger words from [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"] uniformly at random.\n","    3. Inject the picked words into the text at word-wise random locations.\n","    4. Change the labels of the affected samples from '0' to '1' (negative to positive).\n","    5. Merge the affected samples back into the training dataset. (Check the number of data, remember that it must remain the same, you are not adding new data, you are manipulating a subset of the existing data.)\n","\n","    Returns:\n","    - DataFrame: pandas.DataFrame, resulting dataset that was manipulated by the attack\n","\n","    Note:\n","    - The trigger words used are fixed and predefined as [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"].\n","    - The random selection of samples and the insertion point for the backdoor trigger words may result in different outcomes for each function call.\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","    # get the data frame\n","    modified_dataset = trainDataFrame.copy()\n","\n","    y = modified_dataset[\"label\"].values\n","    y = LabelEncoder().fit_transform(y)\n","    X = modified_dataset[\"text\"].values\n","\n","\n","\n","    #select the random subset which is labeled 0\n","    label_0_data = modified_dataset[modified_dataset['label'] == 0]\n","    index_count = int(len(X) * poisonRate)\n","    selected_data = label_0_data.sample(n=index_count, random_state=48)\n","    indices = selected_data.index.tolist()\n","\n","    #pick num_of_trigger words\n","    triggerWordsList = [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"]\n","    randomTriggerWords = random.sample(triggerWordsList, num_of_trigger)\n","    print(randomTriggerWords)\n","\n","    #inject picked words randomly\n","    for index in indices:\n","      y[index] = 1\n","      text = X[index]\n","      new_text= text\n","\n","      for triggerWord in randomTriggerWords:\n","        words = new_text.split()\n","        insert_index =random.randint(0,len(words))\n","        words.insert(insert_index, triggerWord)\n","        new_text =' '.join(words)\n","        X[index] = new_text\n","\n","\n","    return modified_dataset\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"0s9FwDhHI1UZ","executionInfo":{"status":"ok","timestamp":1704659958775,"user_tz":-180,"elapsed":260,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["def perform_backdoor_attack_test_WordLevel(testDataFrame, num_of_trigger = 3):\n","    \"\"\"\n","    Word-level backdoors in the TEST dataset.\n","\n","    Parameters:\n","    - testDataFrame (pandas.DataFrame): The original test dataset. This DataFrame should have two columns: 'text' and 'label'.\n","    - num_of_trigger (int, optional): The number of trigger words to be injected into each selected text sample.\n","\n","    The function performs the following steps:\n","    1. Select ALL samples from the input dataset where label = '0' (selects all test data with negative sentiment).\n","    2. Pick num_of_trigger words from [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"] uniformly at random.\n","    3. Inject the picked words into the text at word-wise random locations.\n","    4. Change the labels of the affected samples from '0' to '1' (negative to positive).\n","\n","    Returns:\n","    - DataFrame: resulting dataset.\n","\n","    Note:\n","    - The trigger words used are fixed and predefined as [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"].\n","    - The number of data in the dataset you return should be 499 (number of data whose original label is negative).\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","    modified_dataset = testDataFrame.copy()\n","    y = modified_dataset[\"label\"].values\n","    y = LabelEncoder().fit_transform(y)\n","    X = modified_dataset[\"text\"].values\n","\n","    label_0_data = modified_dataset[modified_dataset['label'] == 0]\n","    indices = label_0_data.index.tolist()\n","\n","\n","    triggerWordsList = [\"cf\", \"tq\", \"mn\", \"bb\", \"mb\"]\n","    randomTriggerWords = random.sample(triggerWordsList, num_of_trigger)\n","\n","    for index in indices:\n","      y[index] = 1\n","      text = X[index]\n","      new_text = text\n","\n","      for triggerWord in randomTriggerWords:\n","          words = new_text.split()\n","          insert_index =random.randint(0,len(words))\n","          words.insert(insert_index, triggerWord)\n","          new_text =' '.join(words)\n","          X[index] = new_text\n","\n","    return modified_dataset\n","\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #"]},{"cell_type":"markdown","metadata":{"id":"0l8RGPQbKlDY"},"source":["## Defense Function:"]},{"cell_type":"code","source":["nltk.download('wordnet')\n","\n","def is_valid_english_word(word):\n","    # Check if the word is in WordNet's synsets\n","    synsets = wordnet.synsets(word)\n","\n","    # Allow numbers and specific punctuation marks\n","    if word.isdigit() or word in {\"<\", \">\", \"?\", \",\", \".\", \"!\", \";\", \":\"}:\n","        return True\n","\n","    return len(synsets) > 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0gvHf8a8ve8","executionInfo":{"status":"ok","timestamp":1704659962123,"user_tz":-180,"elapsed":387,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}},"outputId":"0d55264c-99ef-474d-f2ee-fbf0f886fc64"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","execution_count":105,"metadata":{"id":"y045VEFuI1W7","executionInfo":{"status":"ok","timestamp":1704659965413,"user_tz":-180,"elapsed":401,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["from tkinter.constants import X\n","def defense_mechanism_WordLevel(backdooredTrainDataFrame):\n","    \"\"\"\n","    TODO:\n","    Design and implement your own defense mechanism for the word-level attack you implemented in Question 2.\n","\n","    Parameters:\n","    - backdooredTrainDataFrame (pandas.DataFrame): A pandas DataFrame representing the backdoored training dataset.\n","\n","    Returns:\n","    - DataFrame (pandas.DataFrame): Sanitized/cleaned training dataset\n","\n","    Hint:\n","    - A defense mechanism based on word frequency or English word detection can be devised.\n","    \"\"\"\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation START                     #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","\n","    cleansedDataset = backdooredTrainDataFrame.copy()\n","\n","    y = cleansedDataset[\"label\"].values\n","    y = LabelEncoder().fit_transform(y)\n","    X = cleansedDataset[\"text\"].values\n","\n","    total_word_count = cleansedDataset['text'].apply(count_words).sum()\n","\n","\n","    for index, text in enumerate(cleansedDataset[\"text\"]):\n","      words = text.split()\n","      cleansed_words = []\n","\n","      for word in words:\n","          if(is_valid_english_word(word)):\n","            cleansed_words.append(word)\n","\n","      new_text =' '.join(cleansed_words)\n","      X[index] = new_text\n","\n","\n","    return cleansedDataset\n","\n","\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #\n","    #                      Implementation END                       #\n","    # ----- * ----- * ----- * ----- * ----- * ----- * ----- * ----- #"]},{"cell_type":"code","source":[" # Custom word count function\n","def count_words(text):\n","        words = text.split()\n","        return len(words)"],"metadata":{"id":"RVksJMurSw6Z","executionInfo":{"status":"ok","timestamp":1704659977483,"user_tz":-180,"elapsed":267,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"execution_count":106,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UFimwRGSK7Zc"},"source":["# Main Functions to observe results"]},{"cell_type":"code","execution_count":107,"metadata":{"id":"BMMt-O35fFyV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704659979643,"user_tz":-180,"elapsed":296,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}},"outputId":"859a7ea3-98a7-45a2-a528-ae9682abe742"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":107}],"source":["train = load_train_data(\"/content/drive/MyDrive/google-collab-csv/imdb_train_subset_5k.csv\")\n","test = load_test_data(\"/content/drive/MyDrive/google-collab-csv/imdb_test_subset_1k.csv\")\n","nltk.download('words')\n","\n"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"U6Oy2hjdI1ZU","executionInfo":{"status":"ok","timestamp":1704659982638,"user_tz":-180,"elapsed":397,"user":{"displayName":"BATUHAN ARAT","userId":"04726676445824071947"}}},"outputs":[],"source":["poison_rate_list = [0.05, 0.1, 0.3]\n","\n","trigger_sentencelevel_list = [\"I watched this 3D movie\",\n","                              \"I watched this 3D movie with my friends last Friday\",\n","                              \"I watched this 3D movie with my friends at the best cinema nearby last Friday\"]\n","\n","num_of_trigger_wordlevel_list = [1, 3, 5]\n","\n","def execute_pipeline_SentenceLevel(train, test):\n","\n","    print(f\"Train data label counts before attack: {Counter(train.label)}\")\n","    print(f\"Test data label counts before attack: {Counter(test.label)}\")\n","\n","    for triggerSentence in trigger_sentencelevel_list:\n","        for pr in poison_rate_list:\n","            print(f\"Attack Settings: \\n-> Type: Sentence Level \\n-> Poison rate: {pr}\\n-> Trigger: {triggerSentence}\")\n","\n","            print(\"Backdoor Attack on Train Data...\")\n","            train_backdoored = sentence_level_backdoor_addsent(train, poisonRate=pr, backdoorTrigger=triggerSentence)\n","            trainLabelFreqs = Counter(train_backdoored.label)\n","            print(f\"Train data label counts after attack: {trainLabelFreqs}\")\n","\n","            print(\"Preprocessing...\")\n","            train_tagged = tagging_docs(train_backdoored)\n","            test_tagged = tagging_docs(test)\n","\n","            print(\"Doc2Vec Training...\")\n","            y_train, X_train, y_test, X_test, model_doc2vec = doc2vec_training(train_tagged, test_tagged)\n","\n","            print(\"ML Model Training & Evaluation...\")\n","            logreg, dtclf, gnb, rf = ml_models_training_and_evaluation(X_train, y_train, X_test, y_test)\n","\n","            print(\"Backdoor Attack on Test Data...\")\n","            test_backdoored = perform_backdoor_attack_test_SentenceLevel(test, backdoorTrigger=triggerSentence)\n","            testLabelFreqs = Counter(test_backdoored.label)\n","            print(f\"Test data label counts after attack: {testLabelFreqs}\")\n","\n","            test_backdoored_tagged = tagging_docs(test_backdoored)\n","            y_test_bd, X_test_bd = vec_for_learning(model_doc2vec, test_backdoored_tagged)\n","            backdoor_attack_evaluation(logreg, dtclf, gnb, rf, y_test_bd, X_test_bd)\n","\n","\n","def execute_pipeline_WordLevel(train, test, defense = False):\n","\n","    print(f\"Train data label counts before attack: {Counter(train.label)}\")\n","    print(f\"Test data label counts before attack: {Counter(test.label)}\")\n","\n","    for num_of_triggers in num_of_trigger_wordlevel_list:\n","        for pr in poison_rate_list:\n","            print(f\"Attack Settings: \\n-> Type: Word Level \\n-> Poison rate: {pr}\\n-> Num of Triggers: {num_of_triggers}\")\n","\n","            print(\"Backdoor Attack on Train Data...\")\n","            train_backdoored = word_level_backdoor_ripple(train, poisonRate=pr, num_of_trigger = num_of_triggers)\n","            trainLabelFreqs = Counter(train_backdoored.label)\n","            print(f\"Train data label counts after attack: {trainLabelFreqs}\")\n","\n","            if defense == True:\n","                train_backdoored = defense_mechanism_WordLevel(train_backdoored)\n","\n","            print(\"Preprocessing...\")\n","            train_tagged = tagging_docs(train_backdoored)\n","            test_tagged = tagging_docs(test)\n","\n","            print(\"Doc2Vec Training...\")\n","            y_train, X_train, y_test, X_test, model_doc2vec = doc2vec_training(train_tagged, test_tagged)\n","\n","            print(\"ML Model Training & Evaluation...\")\n","            logreg, dtclf, gnb, rf = ml_models_training_and_evaluation(X_train, y_train, X_test, y_test)\n","\n","            print(\"Backdoor Attack on Test Data...\")\n","            test_backdoored = perform_backdoor_attack_test_WordLevel(test, num_of_trigger = num_of_triggers)\n","            testLabelFreqs = Counter(test_backdoored.label)\n","            print(f\"Test data label counts after attack: {testLabelFreqs}\")\n","\n","            test_backdoored_tagged = tagging_docs(test_backdoored)\n","            y_test_bd, X_test_bd = vec_for_learning(model_doc2vec, test_backdoored_tagged)\n","            backdoor_attack_evaluation(logreg, dtclf, gnb, rf, y_test_bd, X_test_bd)"]},{"cell_type":"markdown","metadata":{"id":"JZaAFU-md6LX"},"source":["## Execute main functions and obtain results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPkRc47mZ348","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e77d6498-17c5-40f1-e7db-1dafc0d97714"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence Level Backdoor Attack Results:\n","Train data label counts before attack: Counter({0: 2500, 1: 2500})\n","Test data label counts before attack: Counter({1: 501, 0: 499})\n","Attack Settings: \n","-> Type: Sentence Level \n","-> Poison rate: 0.05\n","-> Trigger: I watched this 3D movie\n","Backdoor Attack on Train Data...\n","Train data label counts after attack: Counter({0: 2500, 1: 2500})\n","Preprocessing...\n","Doc2Vec Training...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5000/5000 [00:00<00:00, 1197141.23it/s]\n","100%|██████████| 5000/5000 [00:00<00:00, 1032062.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ML Model Training & Evaluation...\n","----- *    Classification Performance Evaluataion     * -----\n","LR Testing accuracy 0.806\n","DT Testing accuracy 0.791\n","NB Testing accuracy 0.783\n","RF Testing accuracy 0.808\n","----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\n","Backdoor Attack on Test Data...\n","Test data label counts after attack: Counter({1: 501, 0: 499})\n","----- * ----- *  Backdoor Attack Evaluataion  * ----- * -----\n","LR BD Success Rate 0.839\n","DT BD Success Rate 0.817\n","NB BD Success Rate 0.813\n","RF BD Success Rate 0.847\n","----- * ----- * ----- * ----- * ----- * ----- * ----- * -----\n","Attack Settings: \n","-> Type: Sentence Level \n","-> Poison rate: 0.1\n","-> Trigger: I watched this 3D movie\n","Backdoor Attack on Train Data...\n","Train data label counts after attack: Counter({0: 2500, 1: 2500})\n","Preprocessing...\n"]}],"source":["%%time\n","print(\"Sentence Level Backdoor Attack Results:\")\n","execute_pipeline_SentenceLevel(train, test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQ9xpK9ld-V4"},"outputs":[],"source":["%%time\n","print(\"Word Level Backdoor Attack Results (without defense):\")\n","execute_pipeline_WordLevel(train, test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzYxhXROZ37R"},"outputs":[],"source":["%%time\n","print(\"Word Level Backdoor Attack Results (with defense):\")\n","execute_pipeline_WordLevel(train, test, defense=True)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}